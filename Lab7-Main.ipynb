{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Evaluation ðŸš— 2.5 (McQueen Edition)\n",
    "   \n",
    "## Lab Seven: Wide and Deep Network Architectures\n",
    "   \n",
    "### Justin Ledford, Luke Wood, Traian Pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import plotly\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "Select a dataset identically to lab one. That is, the dataset must be table data. In terms of generalization performance, it is helpful to have a large dataset for building a wide and deep network. It is also helpful to have many different categorical features to create the embeddings and cross-product embeddings. It is fine to perform binary classification or multi-class classification.\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/\n",
    "\n",
    "___ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (40 points total)\n",
    "   \n",
    "### [10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). \n",
    "   \n",
    "### [10 points] Identify groups of features in your data that should be combined into cross-product features.\n",
    "   \n",
    "### [10 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "   \n",
    "### [10 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "headers = ['age', 'workclass', 'fnlwgt', 'education', 'education-grade', 'marital-status', 'occupation', 'relationship', \n",
    "              'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "df = pd.read_csv(\n",
    "        'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "        names=headers)\n",
    "df_test = pd.read_csv(\n",
    "        'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test',names=headers)\n",
    "df_test = df_test.ix[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age                32561 non-null int64\n",
      "workclass          32561 non-null object\n",
      "fnlwgt             32561 non-null int64\n",
      "education          32561 non-null object\n",
      "education-grade    32561 non-null int64\n",
      "marital-status     32561 non-null object\n",
      "occupation         32561 non-null object\n",
      "relationship       32561 non-null object\n",
      "race               32561 non-null object\n",
      "sex                32561 non-null object\n",
      "capital-gain       32561 non-null int64\n",
      "capital-loss       32561 non-null int64\n",
      "hours-per-week     32561 non-null int64\n",
      "native-country     32561 non-null object\n",
      "income             32561 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16281 entries, 1 to 16281\n",
      "Data columns (total 15 columns):\n",
      "age                16281 non-null object\n",
      "workclass          16281 non-null object\n",
      "fnlwgt             16281 non-null float64\n",
      "education          16281 non-null object\n",
      "education-grade    16281 non-null float64\n",
      "marital-status     16281 non-null object\n",
      "occupation         16281 non-null object\n",
      "relationship       16281 non-null object\n",
      "race               16281 non-null object\n",
      "sex                16281 non-null object\n",
      "capital-gain       16281 non-null float64\n",
      "capital-loss       16281 non-null float64\n",
      "hours-per-week     16281 non-null float64\n",
      "native-country     16281 non-null object\n",
      "income             16281 non-null object\n",
      "dtypes: float64(5), object(10)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "df_train_deep = deepcopy(df)\n",
    "df_test_deep = deepcopy(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (50 points total)\n",
    "   \n",
    "### [20 points] Create a combined wide and deep network to classify your data using tensorflow.\n",
    "   \n",
    "### [20 points] Investigate generalization performance by altering the number of layers. Try at least two different deep network architectures. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 points] Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (10 points total)\n",
    "   \n",
    "### One idea: Investigate which cross-product features are most important and hypothesize why.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
