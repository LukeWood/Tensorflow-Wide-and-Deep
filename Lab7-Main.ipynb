{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Car Evaluation ðŸš— 2.5 (McQueen Edition)\n",
    "   \n",
    "## Lab Seven: Wide and Deep Network Architectures\n",
    "   \n",
    "### Justin Ledford, Luke Wood, Traian Pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import plotly\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dataset Selection\n",
    "\n",
    "Select a dataset identically to lab one. That is, the dataset must be table data. In terms of generalization performance, it is helpful to have a large dataset for building a wide and deep network. It is also helpful to have many different categorical features to create the embeddings and cross-product embeddings. It is fine to perform binary classification or multi-class classification.\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/\n",
    "\n",
    "___ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preparation (40 points total)\n",
    "   \n",
    "### [10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). \n",
    "   \n",
    "### [10 points] Identify groups of features in your data that should be combined into cross-product features.\n",
    "   \n",
    "### [10 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "   \n",
    "### [10 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "\n",
    "# Get column names\n",
    "r = requests.get('https://raw.githubusercontent.com/LukeWoodSMU/WillBeRenamed/master/col_names.txt')\n",
    "\n",
    "if r.status_code == 200:\n",
    "    columns = r.text.replace(\" \",\"_\").replace(\"'\",\"\").split('\\n')[:-1]\n",
    "else:\n",
    "    print('Error loading column names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'class_of_worker', 'industry_code', 'occupation_code', 'education', 'wage_per_hour', 'enrolled_in_edu_inst_last_wk', 'marital_status', 'major_industry_code', 'major_occupation_code', 'race', 'hispanic_origin', 'sex', 'member_of_a_labor_union', 'reason_for_unemployment', 'full_or_part_time_employment_stat', 'capital_gains', 'capital_losses', 'dividends_from_stocks', 'tax_filer_status', 'region_of_previous_residence', 'state_of_previous_residence', 'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'migration_code-change_in_msa', 'migration_code-change_in_reg', 'migration_code-move_within_reg', 'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt', 'num_persons_worked_for_employer', 'family_members_under_18', 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'citizenship', 'own_business_or_self_employed', 'fill_inc_questionnaire_for_veterans_admin', 'veterans_benefits', 'weeks_worked_in_year', 'year', 'income']\n"
     ]
    }
   ],
   "source": [
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>class_of_worker</th>\n",
       "      <th>industry_code</th>\n",
       "      <th>occupation_code</th>\n",
       "      <th>education</th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>enrolled_in_edu_inst_last_wk</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>major_industry_code</th>\n",
       "      <th>major_occupation_code</th>\n",
       "      <th>...</th>\n",
       "      <th>country_of_birth_father</th>\n",
       "      <th>country_of_birth_mother</th>\n",
       "      <th>country_of_birth_self</th>\n",
       "      <th>citizenship</th>\n",
       "      <th>own_business_or_self_employed</th>\n",
       "      <th>fill_inc_questionnaire_for_veterans_admin</th>\n",
       "      <th>veterans_benefits</th>\n",
       "      <th>weeks_worked_in_year</th>\n",
       "      <th>year</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>Self-employed-not incorporated</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>Some college but no degree</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Precision production craft &amp; repair</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Children</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Children</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>Bachelors degree(BA AB BS)</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Married-civilian spouse present</td>\n",
       "      <td>Finance insurance and real estate</td>\n",
       "      <td>Executive admin and managerial</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>Some college but no degree</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Married-civilian spouse present</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Machine operators assmblrs &amp; inspctrs</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age                  class_of_worker  industry_code  occupation_code  \\\n",
       "1   58   Self-employed-not incorporated              4               34   \n",
       "3    9                  Not in universe              0                0   \n",
       "4   10                  Not in universe              0                0   \n",
       "6   42                          Private             34                3   \n",
       "9   34                          Private              4               37   \n",
       "\n",
       "                     education  wage_per_hour enrolled_in_edu_inst_last_wk  \\\n",
       "1   Some college but no degree              0              Not in universe   \n",
       "3                     Children              0              Not in universe   \n",
       "4                     Children              0              Not in universe   \n",
       "6   Bachelors degree(BA AB BS)              0              Not in universe   \n",
       "9   Some college but no degree              0              Not in universe   \n",
       "\n",
       "                     marital_status                 major_industry_code  \\\n",
       "1                          Divorced                        Construction   \n",
       "3                     Never married         Not in universe or children   \n",
       "4                     Never married         Not in universe or children   \n",
       "6   Married-civilian spouse present   Finance insurance and real estate   \n",
       "9   Married-civilian spouse present                        Construction   \n",
       "\n",
       "                    major_occupation_code    ...     country_of_birth_father  \\\n",
       "1     Precision production craft & repair    ...               United-States   \n",
       "3                         Not in universe    ...               United-States   \n",
       "4                         Not in universe    ...               United-States   \n",
       "6          Executive admin and managerial    ...               United-States   \n",
       "9   Machine operators assmblrs & inspctrs    ...               United-States   \n",
       "\n",
       "  country_of_birth_mother country_of_birth_self  \\\n",
       "1           United-States         United-States   \n",
       "3           United-States         United-States   \n",
       "4           United-States         United-States   \n",
       "6           United-States         United-States   \n",
       "9           United-States         United-States   \n",
       "\n",
       "                          citizenship own_business_or_self_employed  \\\n",
       "1   Native- Born in the United States                             0   \n",
       "3   Native- Born in the United States                             0   \n",
       "4   Native- Born in the United States                             0   \n",
       "6   Native- Born in the United States                             0   \n",
       "9   Native- Born in the United States                             0   \n",
       "\n",
       "  fill_inc_questionnaire_for_veterans_admin  veterans_benefits  \\\n",
       "1                           Not in universe                  2   \n",
       "3                           Not in universe                  0   \n",
       "4                           Not in universe                  0   \n",
       "6                           Not in universe                  2   \n",
       "9                           Not in universe                  2   \n",
       "\n",
       "   weeks_worked_in_year  year     income  \n",
       "1                    52    94   - 50000.  \n",
       "3                     0    94   - 50000.  \n",
       "4                     0    94   - 50000.  \n",
       "6                    52    94   - 50000.  \n",
       "9                    52    94   - 50000.  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\n",
    "        'https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.data.gz',\n",
    "        compression='gzip', header=None, index_col=False)\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "        'https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.test.gz',\n",
    "        compression='gzip', header=None, index_col=False)\n",
    "\n",
    "\n",
    "# Remove weight columns\n",
    "df_train.drop(24, 1, inplace=True)\n",
    "df_test.drop(24, 1, inplace=True)\n",
    "\n",
    "df_train.columns = columns\n",
    "df_test.columns = columns\n",
    "\n",
    "# Remove rows with missing data and reset index\n",
    "df_train.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index()\n",
    "\n",
    "df_test.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index()\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_of_worker', 'industry_code', 'occupation_code', 'education', 'enrolled_in_edu_inst_last_wk', 'marital_status', 'major_industry_code', 'major_occupation_code', 'race', 'hispanic_origin', 'sex', 'member_of_a_labor_union', 'reason_for_unemployment', 'full_or_part_time_employment_stat', 'tax_filer_status', 'region_of_previous_residence', 'state_of_previous_residence', 'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'migration_code-change_in_msa', 'migration_code-change_in_reg', 'migration_code-move_within_reg', 'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt', 'family_members_under_18', 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'citizenship', 'own_business_or_self_employed', 'fill_inc_questionnaire_for_veterans_admin', 'veterans_benefits', 'year']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'wage_per_hour',\n",
       " 'capital_gains',\n",
       " 'capital_losses',\n",
       " 'dividends_from_stocks',\n",
       " 'num_persons_worked_for_employer',\n",
       " 'weeks_worked_in_year']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process data:\n",
    "\n",
    "# check income values consistent (only 2 values)\n",
    "\n",
    "# replace categorical with one hot encoding\n",
    "# scale continuous\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "labels = dict() \n",
    "\n",
    "r = requests.get('https://raw.githubusercontent.com/LukeWoodSMU/WillBeRenamed/master/categorical.txt')\n",
    "categorical_labels = r.text.replace(\" \",\"_\").replace(\"'\",\"\").split('\\n')[:-1]\n",
    "print(categorical_labels)\n",
    "\n",
    "for col in categorical_labels + ['income']:\n",
    "    # strip extra space in strings\n",
    "    if df_train[col].dtype == 'object':\n",
    "        df_train[col] = df_train[col].str.strip()\n",
    "    if df_test[col].dtype == 'object':\n",
    "        df_test[col] = df_test[col].str.strip()\n",
    "        \n",
    "    # convert to ints for one hot encoder to work\n",
    "    \n",
    "    # keep labels for reference \n",
    "    labels[col] = list(set(df_train[col].unique()) | set(df_test[col].unique()))\n",
    "    \n",
    "    df_train[col].replace(to_replace=labels[col],\n",
    "                            value=np.arange(len(labels[col])),\n",
    "                            inplace=True)\n",
    "    df_test[col].replace(to_replace=labels[col],\n",
    "                            value=np.arange(len(labels[col])),\n",
    "                            inplace=True)\n",
    "    \n",
    "    \n",
    "r = requests.get('https://raw.githubusercontent.com/LukeWoodSMU/WillBeRenamed/master/continuous.txt')\n",
    "continuous_labels = r.text.replace(\" \",\"_\").replace(\"'\",\"\").split('\\n')[:-1]\n",
    "\n",
    "\n",
    "for col in continuous_labels:\n",
    "    df_train[col] = df_train[col].astype(np.float32)\n",
    "    df_test[col] = df_test[col].astype(np.float32)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    df_train[col] = ss.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "    df_test[col] = ss.transform(df_test[col].values.reshape(-1, 1))\n",
    "    \n",
    "continuous_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.learn.python import SKCompat\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "tf.logging.set_verbosity(tf.logging.WARN) # control the verbosity of tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's start with the TF example (manipulated to work with new syntax)\n",
    "# https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "def process_input(df, label_header, categ_headers, numeric_headers):\n",
    "    # input: what ever you need it to be\n",
    "    # output: (dict of feature columns as tensors), (labels as tensors)\n",
    "    \n",
    "    # ========Process Inputs=========\n",
    "    # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "    # the values of that column stored in a constant Tensor.\n",
    "    continuous_cols = {k: tf.expand_dims( # make it a column vector\n",
    "                            tf.cast( # cast to a float32\n",
    "                                tf.constant(df[k].values), \n",
    "                                tf.float32), \n",
    "                            1)\n",
    "                       for k in numeric_headers}\n",
    "    \n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored as constant Tensors (numeric)\n",
    "    # then use tensor flow to one hot encode them using the given number of classes \n",
    "    # name of encoder is **_int need to map only to **\n",
    "    categorical_cols = {k: tf.one_hot(indices=tf.constant(df[k].values),\n",
    "                                      depth=len(labels[k])) \n",
    "                        for k in categ_headers}\n",
    "    \n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(continuous_cols)\n",
    "    feature_cols.update(categorical_cols)\n",
    "    \n",
    "    # Convert the label column into a constant Tensor.\n",
    "    label = None\n",
    "    if label_header is not None:\n",
    "        label = tf.constant(df[label_header].values)\n",
    "        \n",
    "    return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'age': <tf.Tensor 'ExpandDims:0' shape=(95130, 1) dtype=float32>,\n",
       "  'capital_gains': <tf.Tensor 'ExpandDims_2:0' shape=(95130, 1) dtype=float32>,\n",
       "  'capital_losses': <tf.Tensor 'ExpandDims_3:0' shape=(95130, 1) dtype=float32>,\n",
       "  'citizenship': <tf.Tensor 'one_hot_28:0' shape=(95130, 5) dtype=float32>,\n",
       "  'class_of_worker': <tf.Tensor 'one_hot:0' shape=(95130, 9) dtype=float32>,\n",
       "  'country_of_birth_father': <tf.Tensor 'one_hot_25:0' shape=(95130, 40) dtype=float32>,\n",
       "  'country_of_birth_mother': <tf.Tensor 'one_hot_26:0' shape=(95130, 40) dtype=float32>,\n",
       "  'country_of_birth_self': <tf.Tensor 'one_hot_27:0' shape=(95130, 41) dtype=float32>,\n",
       "  'detailed_household_and_family_stat': <tf.Tensor 'one_hot_17:0' shape=(95130, 38) dtype=float32>,\n",
       "  'detailed_household_summary_in_household': <tf.Tensor 'one_hot_18:0' shape=(95130, 8) dtype=float32>,\n",
       "  'dividends_from_stocks': <tf.Tensor 'ExpandDims_4:0' shape=(95130, 1) dtype=float32>,\n",
       "  'education': <tf.Tensor 'one_hot_3:0' shape=(95130, 17) dtype=float32>,\n",
       "  'enrolled_in_edu_inst_last_wk': <tf.Tensor 'one_hot_4:0' shape=(95130, 3) dtype=float32>,\n",
       "  'family_members_under_18': <tf.Tensor 'one_hot_24:0' shape=(95130, 5) dtype=float32>,\n",
       "  'fill_inc_questionnaire_for_veterans_admin': <tf.Tensor 'one_hot_30:0' shape=(95130, 3) dtype=float32>,\n",
       "  'full_or_part_time_employment_stat': <tf.Tensor 'one_hot_13:0' shape=(95130, 1) dtype=float32>,\n",
       "  'hispanic_origin': <tf.Tensor 'one_hot_9:0' shape=(95130, 10) dtype=float32>,\n",
       "  'industry_code': <tf.Tensor 'one_hot_1:0' shape=(95130, 52) dtype=float32>,\n",
       "  'live_in_this_house_1_year_ago': <tf.Tensor 'one_hot_22:0' shape=(95130, 3) dtype=float32>,\n",
       "  'major_industry_code': <tf.Tensor 'one_hot_6:0' shape=(95130, 24) dtype=float32>,\n",
       "  'major_occupation_code': <tf.Tensor 'one_hot_7:0' shape=(95130, 15) dtype=float32>,\n",
       "  'marital_status': <tf.Tensor 'one_hot_5:0' shape=(95130, 7) dtype=float32>,\n",
       "  'member_of_a_labor_union': <tf.Tensor 'one_hot_11:0' shape=(95130, 3) dtype=float32>,\n",
       "  'migration_code-change_in_msa': <tf.Tensor 'one_hot_19:0' shape=(95130, 9) dtype=float32>,\n",
       "  'migration_code-change_in_reg': <tf.Tensor 'one_hot_20:0' shape=(95130, 8) dtype=float32>,\n",
       "  'migration_code-move_within_reg': <tf.Tensor 'one_hot_21:0' shape=(95130, 9) dtype=float32>,\n",
       "  'migration_prev_res_in_sunbelt': <tf.Tensor 'one_hot_23:0' shape=(95130, 3) dtype=float32>,\n",
       "  'num_persons_worked_for_employer': <tf.Tensor 'ExpandDims_5:0' shape=(95130, 1) dtype=float32>,\n",
       "  'occupation_code': <tf.Tensor 'one_hot_2:0' shape=(95130, 47) dtype=float32>,\n",
       "  'own_business_or_self_employed': <tf.Tensor 'one_hot_29:0' shape=(95130, 3) dtype=float32>,\n",
       "  'race': <tf.Tensor 'one_hot_8:0' shape=(95130, 5) dtype=float32>,\n",
       "  'reason_for_unemployment': <tf.Tensor 'one_hot_12:0' shape=(95130, 6) dtype=float32>,\n",
       "  'region_of_previous_residence': <tf.Tensor 'one_hot_15:0' shape=(95130, 6) dtype=float32>,\n",
       "  'sex': <tf.Tensor 'one_hot_10:0' shape=(95130, 2) dtype=float32>,\n",
       "  'state_of_previous_residence': <tf.Tensor 'one_hot_16:0' shape=(95130, 50) dtype=float32>,\n",
       "  'tax_filer_status': <tf.Tensor 'one_hot_14:0' shape=(95130, 6) dtype=float32>,\n",
       "  'veterans_benefits': <tf.Tensor 'one_hot_31:0' shape=(95130, 3) dtype=float32>,\n",
       "  'wage_per_hour': <tf.Tensor 'ExpandDims_1:0' shape=(95130, 1) dtype=float32>,\n",
       "  'weeks_worked_in_year': <tf.Tensor 'ExpandDims_6:0' shape=(95130, 1) dtype=float32>,\n",
       "  'year': <tf.Tensor 'one_hot_32:0' shape=(95130, 1) dtype=float32>},\n",
       " <tf.Tensor 'Const_40:0' shape=(95130,) dtype=int64>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_input(df_train,'income',categorical_labels, continuous_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def MLP(dict_features, targets, mode):\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    #=======DECODE FEATURES================\n",
    "    # now let's combine the tensors from the input dictionary\n",
    "    # into a list of the feature columns\n",
    "    features = [dict_features[x] for x in continuous_labels+categorical_labels]\n",
    "   \n",
    "    # also add in the one hot encoded features\n",
    "    for col in categorical_labels:\n",
    "        features.append(dict_features[col])\n",
    "    \n",
    "    # now we can just combine all the features together\n",
    "    features = tf.concat(values=features,axis=1)\n",
    "    \n",
    "    # =====SETUP ARCHITECTURE=====\n",
    "    # we can use functions from learn to add layers and complexity to the model\n",
    "    # pass features through one hidden layer with relu activation\n",
    "    features = layers.relu(features, num_outputs=50) \n",
    "    # now pass the features through a fully connected layer\n",
    "    features = layers.fully_connected(features, num_outputs=1) \n",
    "    # and pass them through a sigmoid activation\n",
    "    output_layer = tf.sigmoid(features) \n",
    "    # reshape the output to be one dimensional\n",
    "    predictions = tf.reshape(output_layer, [-1])\n",
    "    \n",
    "    # depending on the mode, we may not want to evaluate these\n",
    "    loss_mse = None\n",
    "    train_op = None\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        # =====LOSS=======\n",
    "        # we want to use MSE as our loss function\n",
    "        loss_mse = tf.losses.mean_squared_error(targets, predictions) \n",
    "    \n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        # =====OPTIMIZER PARAMS========\n",
    "        # now let's setup how we want thing to optimize \n",
    "        train_op = layers.optimize_loss(\n",
    "            loss=loss_mse, \n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            optimizer='Adagrad', # adaptive gradient, so that the learning rate is not SO important \n",
    "            learning_rate=0.1)\n",
    "    \n",
    "    # what format to have the output in when calling clf.predict?\n",
    "    predictions_out = predictions>0.5\n",
    "    \n",
    "    return model_fn_lib.ModelFnOps(\n",
    "      mode=mode, predictions={'incomes':predictions_out}, loss=loss_mse, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5)\n",
    "\n",
    "df = df_train.ix[:1000].copy()\n",
    "#df = df_train.copy()\n",
    "\n",
    "X = df.drop('income', axis=1).values\n",
    "y = df['income'].values\n",
    "\n",
    "\n",
    "# High cost for false negatives on \"> 50,000\" class\n",
    "\n",
    "def weighted_recall_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Weight determined by class imbalance\n",
    "    \"\"\"\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    counts = np.bincount(y_true)\n",
    "    weight = int(1 / (min(counts) / (sum(counts))))\n",
    "    \n",
    "    weight_matrix = np.array([\n",
    "        [0, weight],\n",
    "        [1, 0]\n",
    "    ])\n",
    "                 \n",
    "    tp = np.sum(np.diagonal(conf_matrix))\n",
    "    fn = np.sum(weight_matrix * conf_matrix)\n",
    "\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpt1wmu8d4\n",
      "[[  1  14]\n",
      " [  0 217]] 0.509345794393\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpjp_53hh5\n",
      "[[  4  11]\n",
      " [  2 215]] 0.567357512953\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp1u3ksv4l\n",
      "[[  2  13]\n",
      " [  3 214]] 0.521739130435\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpjfvzxlx0\n",
      "[[ 15   0]\n",
      " [217   0]] 0.0646551724138\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_6fb5p1_\n",
      "[[  4  11]\n",
      " [  2 215]] 0.567357512953\n",
      "CPU times: user 28 s, sys: 553 ms, total: 28.6 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    clf = learn.Estimator(model_fn=MLP)\n",
    "\n",
    "    # when we provide the process function, they expect us to control the mini-batch\n",
    "    clf.fit(input_fn=\n",
    "                lambda:process_input(df.iloc[train_index], 'income',categorical_labels, continuous_labels), \n",
    "                steps=500)\n",
    "    \n",
    "    y_test = df['income'].iloc[test_index]\n",
    "\n",
    "    yhat = clf.predict(input_fn=\n",
    "                       lambda:process_input(df.iloc[test_index], None,categorical_labels, continuous_labels))\n",
    "    # the output is now an iterable value, so we need to step over it\n",
    "    yhat = [x['incomes'] for x in yhat]\n",
    "    print(confusion_matrix(y_test,yhat),\n",
    "          weighted_recall_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "def process_input_cc(df, label_header, categ_headers, numeric_headers):\n",
    "    # input: what ever you need it to be\n",
    "    # output: (dict of feature columns as tensors), (labels as tensors)\n",
    "    \n",
    "    # ========Process Inputs=========\n",
    "    # not much changes here, except we leave the numerics as tc.constants\n",
    "    continuous_cols = {k: tf.reshape(tf.constant(df[k].values), [-1]) for k in numeric_headers}\n",
    "      \n",
    "    # and we shift these tensors to be sparse one-hot encoded values\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "                              indices=[[i, 0] for i in range(df[k].size)],\n",
    "                              values=df[k].astype(str).values,\n",
    "                              dense_shape=[df[k].size, 1])\n",
    "                        for k in categ_headers}\n",
    "    \n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(categorical_cols)\n",
    "    feature_cols.update(continuous_cols)\n",
    "    \n",
    "    # Convert the label column into a constant Tensor.\n",
    "    label = None\n",
    "    if label_header is not None:\n",
    "        label = tf.constant(df[label_header].values)\n",
    "        \n",
    "    return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_wide_deep_columns():\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    wide_columns = []\n",
    "    deep_columns = []\n",
    "    # add in each of the categorical columns to both wide and deep features\n",
    "    for col in categorical_labels:\n",
    "        wide_columns.append(\n",
    "            layers.sparse_column_with_keys(col, keys=[str(x) for x in labels[col]])\n",
    "        )\n",
    "        \n",
    "        dim = round(np.log2(len(labels[col]))) + 1\n",
    "        deep_columns.append(\n",
    "            layers.embedding_column(wide_columns[-1], dimension=dim)\n",
    "        )\n",
    "        \n",
    "    # also add in some specific crossed columns\n",
    "    cross_columns = [('education','occupation_code'),('country_of_birth_father', 'country_of_birth_mother')]\n",
    "    for tup in cross_columns:\n",
    "        wide_columns.append(\n",
    "            layers.crossed_column(\n",
    "                [layers.sparse_column_with_keys(tup[0], keys=[str(x) for x in labels[tup[0]]]),\n",
    "                 layers.sparse_column_with_keys(tup[1], keys=[str(x) for x in labels[tup[1]]])],\n",
    "            hash_bucket_size=int(1e4))\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # and add in the regular dense features \n",
    "    for col in continuous_labels:\n",
    "        deep_columns.append(\n",
    "            layers.real_valued_column(col)\n",
    "        )\n",
    "                    \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  13]\n",
      " [  1 216]] 0.526570048309\n",
      "[[  3  12]\n",
      " [  0 217]] 0.55\n",
      "[[  3  12]\n",
      " [  0 217]] 0.55\n",
      "[[  1  14]\n",
      " [  0 217]] 0.509345794393\n",
      "[[  3  12]\n",
      " [  1 216]] 0.5475\n",
      "CPU times: user 2min 12s, sys: 1.88 s, total: 2min 14s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "df = df_train.ix[:1000].copy()\n",
    "#df = df_train.copy()\n",
    "\n",
    "X = df.drop('income', axis=1).values\n",
    "y = df['income'].values\n",
    "\n",
    "wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "\n",
    "for col in deep_columns:\n",
    "    if col.dimension == 0.0:\n",
    "        print(col)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    clf = learn.DNNLinearCombinedClassifier(\n",
    "                            linear_feature_columns=wide_columns,\n",
    "                            dnn_feature_columns=deep_columns,\n",
    "                            dnn_hidden_units=[100, 50])\n",
    "\n",
    "    #clf = learn.LinearClassifier(feature_columns=wide_columns)\n",
    "    #clf = learn.DNNClassifier(feature_columns=deep_columns, hidden_units=[100,50])\n",
    "\n",
    "\n",
    "    input_wrapper = lambda:process_input_cc(df.iloc[train_index],'income',categorical_labels, continuous_labels)\n",
    "    output_wrapper = lambda:process_input_cc(df.iloc[test_index],None,categorical_labels, continuous_labels)\n",
    "\n",
    "    # when we provide the process function, they expect us to control the mini-batch\n",
    "    clf.fit(input_fn=input_wrapper, steps=500)\n",
    "\n",
    "    y_test = df['income'].iloc[test_index]\n",
    "\n",
    "    yhat = clf.predict(input_fn=output_wrapper)\n",
    "\n",
    "    # the output is now an iterable value, so we need to step over it\n",
    "    yhat = [x for x in yhat]\n",
    "\n",
    "    print(confusion_matrix(y_test,yhat),\n",
    "          weighted_recall_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Modeling (50 points total)\n",
    "   \n",
    "### [20 points] Create a combined wide and deep network to classify your data using tensorflow.\n",
    "   \n",
    "### [20 points] Investigate generalization performance by altering the number of layers. Try at least two different deep network architectures. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab.\n",
    "\n",
    "### 10 points] Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exceptional Work (10 points total)\n",
    "   \n",
    "### One idea: Investigate which cross-product features are most important and hypothesize why.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
