{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Seizing the Means of Tenserflow ☭\n",
    "   \n",
    "## Lab Seven: Wide and Deep Network Architectures\n",
    "   \n",
    "### Justin Ledford, Luke Wood, Traian Pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import plotly\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dataset Selection\n",
    "\n",
    "For this project, we chose the Census-Income-MLD dataset from the UCI Machine Learning Repository due to nature of the set being exactly what we needed. The set has both categorical and integer data, a large number of attributes (40), and a very large number of instances (approximately 300,000). Not only that, but the data is also very applicable and highly wanted in our society.\n",
    "   \n",
    "The dataset's goal is to be able to predict whether a person, based on the values of their attributes, is under or over the 50k income bar. However, our goal for this report is not simply just accuracy. We are placing much heavier emphasis on catching false negatives in the over 50k limit section. The reason for this choise falls under the logic of marketing and advertising. Since the census is public knowledge, advertising companies tend to use it to be able to predict their target market and where they are most likely to have an affect. \n",
    "   \n",
    "One target for these companies are individuals who make more than the average amount for they have more money to spend. If an advertiser is given a list that is missing a good deal of >50k individuals, they are also missing out on potential profit. This is something that our report will prioritize in order for a situation like that to be avoided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preparation (40 points total)\n",
    "   \n",
    "### [10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "\n",
    "# Get column names\n",
    "r = requests.get('https://raw.githubusercontent.com/LukeWoodSMU/WillBeRenamed/master/col_names.txt')\n",
    "\n",
    "if r.status_code == 200:\n",
    "    columns = r.text.replace(\" \",\"_\").replace(\"'\",\"\").split('\\n')[:-1]\n",
    "else:\n",
    "    print('Error loading column names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'class_of_worker', 'industry_code', 'occupation_code', 'education', 'wage_per_hour', 'enrolled_in_edu_inst_last_wk', 'marital_status', 'major_industry_code', 'major_occupation_code', 'race', 'hispanic_origin', 'sex', 'member_of_a_labor_union', 'reason_for_unemployment', 'full_or_part_time_employment_stat', 'capital_gains', 'capital_losses', 'dividends_from_stocks', 'tax_filer_status', 'region_of_previous_residence', 'state_of_previous_residence', 'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'migration_code-change_in_msa', 'migration_code-change_in_reg', 'migration_code-move_within_reg', 'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt', 'num_persons_worked_for_employer', 'family_members_under_18', 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'citizenship', 'own_business_or_self_employed', 'fill_inc_questionnaire_for_veterans_admin', 'veterans_benefits', 'weeks_worked_in_year', 'year', 'income']\n"
     ]
    }
   ],
   "source": [
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see, there is a large number of attributes, with categories ranging anywhere from age to benefits. In order to ease the work on our program and receive better results, we will be crossing some of these attributes together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section, we remove any attributes with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>class_of_worker</th>\n",
       "      <th>industry_code</th>\n",
       "      <th>occupation_code</th>\n",
       "      <th>education</th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>enrolled_in_edu_inst_last_wk</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>major_industry_code</th>\n",
       "      <th>major_occupation_code</th>\n",
       "      <th>...</th>\n",
       "      <th>country_of_birth_father</th>\n",
       "      <th>country_of_birth_mother</th>\n",
       "      <th>country_of_birth_self</th>\n",
       "      <th>citizenship</th>\n",
       "      <th>own_business_or_self_employed</th>\n",
       "      <th>fill_inc_questionnaire_for_veterans_admin</th>\n",
       "      <th>veterans_benefits</th>\n",
       "      <th>weeks_worked_in_year</th>\n",
       "      <th>year</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>Self-employed-not incorporated</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>Some college but no degree</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Precision production craft &amp; repair</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Children</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Children</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>Bachelors degree(BA AB BS)</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Married-civilian spouse present</td>\n",
       "      <td>Finance insurance and real estate</td>\n",
       "      <td>Executive admin and managerial</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>Some college but no degree</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Married-civilian spouse present</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Machine operators assmblrs &amp; inspctrs</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age                  class_of_worker  industry_code  occupation_code  \\\n",
       "1   58   Self-employed-not incorporated              4               34   \n",
       "3    9                  Not in universe              0                0   \n",
       "4   10                  Not in universe              0                0   \n",
       "6   42                          Private             34                3   \n",
       "9   34                          Private              4               37   \n",
       "\n",
       "                     education  wage_per_hour enrolled_in_edu_inst_last_wk  \\\n",
       "1   Some college but no degree              0              Not in universe   \n",
       "3                     Children              0              Not in universe   \n",
       "4                     Children              0              Not in universe   \n",
       "6   Bachelors degree(BA AB BS)              0              Not in universe   \n",
       "9   Some college but no degree              0              Not in universe   \n",
       "\n",
       "                     marital_status                 major_industry_code  \\\n",
       "1                          Divorced                        Construction   \n",
       "3                     Never married         Not in universe or children   \n",
       "4                     Never married         Not in universe or children   \n",
       "6   Married-civilian spouse present   Finance insurance and real estate   \n",
       "9   Married-civilian spouse present                        Construction   \n",
       "\n",
       "                    major_occupation_code    ...     country_of_birth_father  \\\n",
       "1     Precision production craft & repair    ...               United-States   \n",
       "3                         Not in universe    ...               United-States   \n",
       "4                         Not in universe    ...               United-States   \n",
       "6          Executive admin and managerial    ...               United-States   \n",
       "9   Machine operators assmblrs & inspctrs    ...               United-States   \n",
       "\n",
       "  country_of_birth_mother country_of_birth_self  \\\n",
       "1           United-States         United-States   \n",
       "3           United-States         United-States   \n",
       "4           United-States         United-States   \n",
       "6           United-States         United-States   \n",
       "9           United-States         United-States   \n",
       "\n",
       "                          citizenship own_business_or_self_employed  \\\n",
       "1   Native- Born in the United States                             0   \n",
       "3   Native- Born in the United States                             0   \n",
       "4   Native- Born in the United States                             0   \n",
       "6   Native- Born in the United States                             0   \n",
       "9   Native- Born in the United States                             0   \n",
       "\n",
       "  fill_inc_questionnaire_for_veterans_admin  veterans_benefits  \\\n",
       "1                           Not in universe                  2   \n",
       "3                           Not in universe                  0   \n",
       "4                           Not in universe                  0   \n",
       "6                           Not in universe                  2   \n",
       "9                           Not in universe                  2   \n",
       "\n",
       "   weeks_worked_in_year  year     income  \n",
       "1                    52    94   - 50000.  \n",
       "3                     0    94   - 50000.  \n",
       "4                     0    94   - 50000.  \n",
       "6                    52    94   - 50000.  \n",
       "9                    52    94   - 50000.  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\n",
    "        'https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.data.gz',\n",
    "        compression='gzip', header=None, index_col=False)\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "        'https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.test.gz',\n",
    "        compression='gzip', header=None, index_col=False)\n",
    "\n",
    "\n",
    "# Remove weight columns\n",
    "df_train.drop(24, 1, inplace=True)\n",
    "df_test.drop(24, 1, inplace=True)\n",
    "\n",
    "df_train.columns = columns\n",
    "df_test.columns = columns\n",
    "\n",
    "# Remove rows with missing data and reset index\n",
    "df_train.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index()\n",
    "\n",
    "df_test.replace(to_replace=' ?',value=np.nan, inplace=True)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index()\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wage per hour to categorical\n",
    "df_train[\"wage_per_hour\"] = df_train[\"wage_per_hour\"].map(lambda i: i==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we start encoding our remaining attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class_of_worker',\n",
       " 'industry_code',\n",
       " 'occupation_code',\n",
       " 'education',\n",
       " 'enrolled_in_edu_inst_last_wk',\n",
       " 'marital_status',\n",
       " 'major_industry_code',\n",
       " 'major_occupation_code',\n",
       " 'race',\n",
       " 'hispanic_origin',\n",
       " 'sex',\n",
       " 'member_of_a_labor_union',\n",
       " 'reason_for_unemployment',\n",
       " 'full_or_part_time_employment_stat',\n",
       " 'tax_filer_status',\n",
       " 'region_of_previous_residence',\n",
       " 'state_of_previous_residence',\n",
       " 'detailed_household_and_family_stat',\n",
       " 'detailed_household_summary_in_household',\n",
       " 'migration_code-change_in_msa',\n",
       " 'migration_code-change_in_reg',\n",
       " 'migration_code-move_within_reg',\n",
       " 'live_in_this_house_1_year_ago',\n",
       " 'migration_prev_res_in_sunbelt',\n",
       " 'family_members_under_18',\n",
       " 'country_of_birth_father',\n",
       " 'country_of_birth_mother',\n",
       " 'country_of_birth_self',\n",
       " 'citizenship',\n",
       " 'own_business_or_self_employed',\n",
       " 'fill_inc_questionnaire_for_veterans_admin',\n",
       " 'veterans_benefits',\n",
       " 'year']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process data:\n",
    "\n",
    "# check income values consistent (only 2 values)\n",
    "\n",
    "# replace categorical with one hot encoding\n",
    "# scale continuous\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "labels = dict() \n",
    "\n",
    "r = requests.get('https://raw.githubusercontent.com/LukeWoodSMU/WillBeRenamed/master/categorical.txt')\n",
    "categorical_labels = r.text.replace(\" \",\"_\").replace(\"'\",\"\").split('\\n')[:-1]\n",
    "\n",
    "for col in categorical_labels + ['income']:\n",
    "    # strip extra space in strings\n",
    "    if df_train[col].dtype == 'object':\n",
    "        df_train[col] = df_train[col].str.strip()\n",
    "    if df_test[col].dtype == 'object':\n",
    "        df_test[col] = df_test[col].str.strip()\n",
    "        \n",
    "    # convert to ints for one hot encoder to work\n",
    "    \n",
    "    # keep labels for reference \n",
    "    labels[col] = list(set(df_train[col].unique()) | set(df_test[col].unique()))\n",
    "    \n",
    "    df_train[col].replace(to_replace=labels[col],\n",
    "                            value=np.arange(len(labels[col])),\n",
    "                            inplace=True)\n",
    "    df_test[col].replace(to_replace=labels[col],\n",
    "                            value=np.arange(len(labels[col])),\n",
    "                            inplace=True)\n",
    "\n",
    "categorical_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Above are our current categorical attributes.\n",
    "   \n",
    "Ideas for merging:\n",
    "Country of birth categories too repetitive.\n",
    "Industry and occupation code unreadable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'wage_per_hour',\n",
       " 'capital_gains',\n",
       " 'capital_losses',\n",
       " 'dividends_from_stocks',\n",
       " 'num_persons_worked_for_employer',\n",
       " 'weeks_worked_in_year']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('https://raw.githubusercontent.com/LukeWoodSMU/WillBeRenamed/master/continuous.txt')\n",
    "continuous_labels = r.text.replace(\" \",\"_\").replace(\"'\",\"\").split('\\n')[:-1]\n",
    "\n",
    "\n",
    "for col in continuous_labels:\n",
    "    df_train[col] = df_train[col].astype(np.float32)\n",
    "    df_test[col] = df_test[col].astype(np.float32)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    df_train[col] = ss.fit_transform(df_train[col].values.reshape(-1, 1))\n",
    "    df_test[col] = ss.transform(df_test[col].values.reshape(-1, 1))\n",
    "    \n",
    "continuous_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Above are our current continuous attributes. \n",
    "   \n",
    "Ideas for merging:\n",
    "Turn most of these into categorical. Ranges like the Titanic dataset he showed us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### [10 points] Choose and explain what metric(s) you will use to evaluate your algorithm’s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# High cost for false negatives on \"> 50,000\" class\n",
    "\n",
    "def weighted_recall_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Weight determined by class imbalance\n",
    "    \"\"\"\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    counts = np.bincount(y_true)\n",
    "    weight = int(1 / (min(counts) / (sum(counts))))\n",
    "    \n",
    "    weight_matrix = np.array([\n",
    "        [0, weight],\n",
    "        [1, 0]\n",
    "    ])\n",
    "                 \n",
    "    tp = np.sum(np.diagonal(conf_matrix))\n",
    "    fn = np.sum(weight_matrix * conf_matrix)\n",
    "\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As mentioned above, we are focusing our score success on whether or not we can succesfully predict the false negatives in the >50k class. We implement this by overwriting the recall_score method with a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### [10 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5)\n",
    "\n",
    "df = df_train.ix[:1000].copy()\n",
    "#df = df_train.copy()\n",
    "\n",
    "X = df.drop('income', axis=1).values\n",
    "y = df['income'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The reason we choose Stratified Shuffle Split instead of the others is due to how large our dataset is and it is the recommended one by the source where we received our data from.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Modeling (50 points total)\n",
    "   \n",
    "### [20 points] Create a combined wide and deep network to classify your data using tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.learn.python import SKCompat\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "tf.logging.set_verbosity(tf.logging.WARN) # control the verbosity of tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/wide_and_deep\n",
    "def process_input_cc(df, label_header, categ_headers, numeric_headers):\n",
    "    # input: what ever you need it to be\n",
    "    # output: (dict of feature columns as tensors), (labels as tensors)\n",
    "    \n",
    "    # ========Process Inputs=========\n",
    "    # not much changes here, except we leave the numerics as tc.constants\n",
    "    continuous_cols = {k: tf.reshape(tf.constant(df[k].values), [-1]) for k in numeric_headers}\n",
    "      \n",
    "    # and we shift these tensors to be sparse one-hot encoded values\n",
    "    # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "    # to the values of that column stored in a tf.SparseTensor.\n",
    "    categorical_cols = {k: tf.SparseTensor(\n",
    "                              indices=[[i, 0] for i in range(df[k].size)],\n",
    "                              values=df[k].astype(str).values,\n",
    "                              dense_shape=[df[k].size, 1])\n",
    "                        for k in categ_headers}\n",
    "    \n",
    "    # Merges the two dictionaries into one.\n",
    "    feature_cols = dict(categorical_cols)\n",
    "    feature_cols.update(continuous_cols)\n",
    "    \n",
    "    # Convert the label column into a constant Tensor.\n",
    "    label = None\n",
    "    if label_header is not None:\n",
    "        label = tf.constant(df[label_header].values)\n",
    "        \n",
    "    return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the model to take input features as a dictionary\n",
    "def setup_wide_deep_columns():\n",
    "    # the prototype for this function is as follows\n",
    "    # input:  (features, targets) \n",
    "    # output: (predictions, loss, train_op)\n",
    "    \n",
    "    wide_columns = []    \n",
    "    deep_columns = []\n",
    "    # add in each of the categorical columns to both wide and deep features\n",
    "    for col in categorical_labels:\n",
    "        wide_columns.append(\n",
    "            layers.sparse_column_with_keys(col, keys=[str(x) for x in labels[col]])\n",
    "        )\n",
    "        \n",
    "        dim = round(np.log2(len(labels[col]))) + 1\n",
    "        deep_columns.append(\n",
    "            layers.embedding_column(wide_columns[-1], dimension=dim)\n",
    "        )\n",
    "        \n",
    "    # also add in some specific crossed columns\n",
    "    cross_columns = [('education','occupation_code'),('country_of_birth_father', 'country_of_birth_mother')]\n",
    "    for tup in cross_columns:\n",
    "        wide_columns.append(\n",
    "            layers.crossed_column(\n",
    "                [layers.sparse_column_with_keys(tup[0], keys=[str(x) for x in labels[tup[0]]]),\n",
    "                 layers.sparse_column_with_keys(tup[1], keys=[str(x) for x in labels[tup[1]]])],\n",
    "            hash_bucket_size=int(1e4))\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # and add in the regular dense features \n",
    "    for col in continuous_labels:\n",
    "        deep_columns.append(\n",
    "            layers.real_valued_column(col)\n",
    "        )\n",
    "                    \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Investigate generalization performance by altering the number of layers. Try at least two different deep network architectures. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from numpy import interp\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "df = df_train.ix[:1000].copy()\n",
    "#df = df_train.copy()\n",
    "\n",
    "X = df.drop('income', axis=1).values\n",
    "y = df['income'].values\n",
    "\n",
    "wide_columns, deep_columns = setup_wide_deep_columns()\n",
    "\n",
    "for col in deep_columns:\n",
    "    if col.dimension == 0.0:\n",
    "        print(col)\n",
    "\n",
    "        \n",
    "mlp_roc_auc = []\n",
    "dnn_roc_auc = []\n",
    "\n",
    "mean_tpr_mlp = 0.0\n",
    "mean_fpr_mlp = np.linspace(0, 1, 100)\n",
    "mean_tpr_dnn = 0.0\n",
    "mean_fpr_dnn = np.linspace(0, 1, 100)\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.5)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    dnn = learn.DNNLinearCombinedClassifier(\n",
    "                            linear_feature_columns=wide_columns,\n",
    "                            dnn_feature_columns=deep_columns,\n",
    "                            dnn_hidden_units=[100, 50, 25, 5])\n",
    "    \n",
    "    mlp = MLPClassifier()\n",
    "    \n",
    "    X_train = df.drop('income', axis=1).iloc[train_index]\n",
    "    y_train = df['income'].iloc[train_index]\n",
    "\n",
    "    X_test = df.drop('income', axis=1).iloc[test_index]\n",
    "    y_test = df['income'].iloc[test_index]\n",
    " \n",
    "    input_wrapper = lambda:process_input_cc(df.iloc[train_index],'income',categorical_labels, continuous_labels)\n",
    "    output_wrapper = lambda:process_input_cc(df.iloc[test_index],None,categorical_labels, continuous_labels)\n",
    "\n",
    "    # when we provide the process function, they expect us to control the mini-batch\n",
    "    dnn.fit(input_fn=input_wrapper, steps=500)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    yhat_dnn = list(dnn.predict(input_fn=output_wrapper))\n",
    "    yhat_mlp = mlp.predict(X_test)\n",
    "\n",
    "    print('DNN')\n",
    "    print(confusion_matrix(y_test,yhat_dnn),\n",
    "          accuracy_score(y_test,yhat_dnn))\n",
    "    \n",
    "    print('MLP')\n",
    "    print(confusion_matrix(y_test,yhat_mlp),\n",
    "          accuracy_score(y_test,yhat_mlp))\n",
    "    \n",
    "    \n",
    "    # TODO: fix mean\n",
    "    probs_dnn = list(dnn.predict_proba(input_fn=output_wrapper))\n",
    "    probs_dnn = np.array(probs_dnn)[:,1]\n",
    "    fpr_dnn, tpr_dnn, _ = roc_curve(y_test, probs_dnn)\n",
    "    dnn_roc_auc.append((fpr_dnn, tpr_dnn, auc(fpr_dnn, tpr_dnn)))\n",
    "    mean_tpr_dnn += interp(mean_fpr_dnn, fpr_dnn, tpr_dnn)\n",
    "    mean_tpr_dnn[0] = 0.0   \n",
    "    \n",
    "    probs_mlp = mlp.predict_proba(X_test)\n",
    "    probs_mlp = np.array(probs_mlp)[:,1]\n",
    "    fpr_mlp, tpr_mlp, _ = roc_curve(y_test, probs_mlp)\n",
    "    mlp_roc_auc.append((fpr_mlp, tpr_mlp, auc(fpr_mlp, tpr_mlp)))\n",
    "    mean_tpr_mlp  += interp(mean_fpr_mlp, fpr_mlp, tpr_mlp)\n",
    "    mean_tpr_mlp [0] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_tpr_mlp /= n_splits\n",
    "mean_tpr_mlp[-1] = 1.0\n",
    "mean_auc_mlp = auc(mean_fpr_mlp, mean_tpr_mlp)\n",
    "\n",
    "mean_tpr_dnn /= n_splits\n",
    "mean_tpr_dnn[-1] = 1.0\n",
    "mean_auc_dnn = auc(mean_fpr_dnn, mean_tpr_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 10 points] Compare the performance of your best wide and deep network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "i = 0\n",
    "for fpr, tpr, auc_ in dnn_roc_auc:\n",
    "        plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve # %d (DNN) (area = %0.2f)' % (i, auc_))\n",
    "        i += 1\n",
    "for fpr, tpr, auc_ in mlp_roc_auc:\n",
    "        plt.plot(fpr, tpr, color='green',\n",
    "             lw=lw, label='ROC curve # %d (MLP) (area = %0.2f)' % (i, auc_))\n",
    "        i += 1        \n",
    "        \n",
    "# TODO: fix means\n",
    "# plt.plot(mean_fpr_mlp, mean_tpr_mlp, 'k--', label='mean ROC (MLP) (area = %0.2f)' % mean_auc_mlp, lw=2)\n",
    "# plt.plot(mean_fpr_dnn, mean_tpr_dnn, 'k-', label='mean ROC (DNN) (area = %0.2f)' % mean_auc_dnn, lw=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exceptional Work (10 points total)\n",
    "   \n",
    "### One idea: Investigate which cross-product features are most important and hypothesize why.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def expected_data(observed):\n",
    "    expected = np.zeros(observed.shape)\n",
    "\n",
    "    total = observed.sum().sum()\n",
    "    for j in [0, 1]:\n",
    "        for i, col_total in enumerate(observed.sum()):\n",
    "            row_total = observed.sum(axis=1)[j]\n",
    "            expected[j][i] = row_total*col_total/total\n",
    "\n",
    "    return pd.DataFrame(expected, index=observed.index,\n",
    "                        columns=observed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_freq = df_train.copy()\n",
    "def attr_freqs(attr1, attr2):\n",
    "    return pd.crosstab(df_freq[attr2], df_freq[attr1], rownames=[attr2], colnames=[attr1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chisqrs = []\n",
    "\n",
    "for colname in df_freq.columns:\n",
    "    if colname != 'income':\n",
    "        observed = attr_freqs(colname, 'income')\n",
    "        expected = expected_data(observed)\n",
    "        chisqr = (((observed-expected)**2)/expected).sum().sum()\n",
    "        chisqrs.append((chisqr, colname))\n",
    "\n",
    "chisqrs = sorted(chisqrs)[::-1]\n",
    "chisqrs = chisqrs[:20]\n",
    "values = [d[0] for d in chisqrs]\n",
    "labels = [d[1].replace(\"_\", \"\\n\") for d in chisqrs]\n",
    "\n",
    "index = np.arange(len(chisqrs))\n",
    "bar_width = .35\n",
    "opacity=0.4\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(\"Attributes most associated with income\")\n",
    "plt.bar(index, values, bar_width, align='center')\n",
    "plt.xticks(index, labels)\n",
    "plt.ylabel(\"Chi-squared values\")\n",
    "plt.xlabel(\"Attributes\")\n",
    "plt.autoscale()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
